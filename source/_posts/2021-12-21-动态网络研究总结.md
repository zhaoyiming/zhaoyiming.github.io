---
title: 动态网络研究总结  
date: 2021-12-21 20:13:20
tags:
- Dynamic Network
- 模型压缩
categories: 
- Dynamic Network
---
# 动态网络

模型压缩包括量化、网络剪枝、模型蒸馏和网络结构搜索(NAS)，对于轻量化设备部署神经网络比较有意义。网络剪枝包括动态剪枝和静态剪枝，目的为减少计算量以及模型容量。静态剪枝为主流，通过先确定最优网络然后进行物理修建网络以实现网络瘦身，缺点是对于新的样本效果可能不好。动态剪枝类似于NAS，实际效果有待评估。

## 为什么要做动态网络

+ 传统网络只根据固定权重来推理每个样本，不同难度的样本(一张图像中，占画面很大的人和占画面很小的狗需要的网络深度和宽度都不同)输入时会造成资源浪费。
+ 需要更深的网络，但是保持计算资源不大幅度增加。因为更深的网络有助于网络表达和细粒度推理（更深的网络推理更细粒度的细节）。
+ 在移动网络下，我们需要节省更多的资源，动态网络可以帮助减少计算所需资源。

## 动态网络分类

### Dynamic Network

动态网络包括动态剪枝，也几乎涵括本文中其他主题，请看第一篇survey继续梳理思路。

#### Survey类论文

+ Dynamic Neural Networks A Survey 非常重要，几乎涵括所有类型的动态网络，先看这篇对所有类型动态网络有一个基本了解。
+ Pruning Algorithms to Accelerate Convolutional Neural Networks for Edge 暂时没看，需要时可以参考
+ Pruning and quantization for deep neural network acceleration A survey 同上

### Mixture of expert

目的大部分是为了通过不增加太多计算的情况下提升网络的可表达性(等同于网络容量)。

此篇结合知乎理解一些基础：

+ OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER

此篇实验已做，但效果存在疑问，目前只有arxiv，github无代码，我自己改动实现：

+ Deep Mixture of Experts via Shallow Embedding

传统Moe一般结合transformer(有局限性)，由于transformer的encoder在注意力层后面接一层全连接层，所以比较适合将专家层安排在此层。该篇思想比较新颖，通过结合动态网络的思想，将每层网络（每一层卷积层）视为一个gate网络，该层每一个通道视为一个专家。这样整个CNN网络被视为拥有N个gate网络的动态网络，每一层通道都会根据样本来动态选择专家(也就是每一个通道的权重)。

这种方法的缺点是采用掩模(mask)，通过gate网络生成0和1数列来掩蔽整个卷积层，但是这种方法带来的稀疏性，也就是0不均匀分布(现有机制下，就算我们在该通道事先生成了0，但是我没法控制这个卷积核不进行计算)，是无法实现实际上的加速效果的。

上一篇论文借鉴的是动态剪枝的内容：

+ runtime-neural-pruning

该篇虽然是动态训练网络掩模，但是是训练后又剪枝，如果看不懂上一篇，可以看这一篇理解掩模和动态剪纸的基础。

基于以上两篇，我看到了CVPR2021的一篇新内容，github有代码

+ Manifold Regularized Dynamic Network Pruning

idea非常好，是对于gate网络针对不同难度样本采用不同权重等等，但是可能也没有加速效果。

### Slim Network

目的是为了实际加速，缺点是不减少模型容量。

由于动态网络和混合专家系统拥有稀疏性，但是目前的cuda以及pytroch对稀疏性的支持并不好，其实很难实现真正物理上的加速。Slim network从物理上连续地对卷积核进行掩蔽，所以从理论上可以实现真正的加速，如果要实现真正的效果，要注意这块内容。

*先看这个进行粗略了解。*

如何评价论文：Slimmable Neural Networks？ - 月臻的回答 - 知乎 https://www.zhihu.com/question/306865592/answer/872860400

#### Slim三部曲

+ slimmable_neural_networks
+ Universally_Slimmable_Networks_and_Improved_Training_Techniques
+ AutoSlim: Towards One-Shot Architecture Search for Channel Numbers 无附件。

重点关注前两部，第一部基础，第二部重大改进。

#### 新型Slim

+ dynamic slim 

cvpr2021的一篇新作品，结合动态网络和slim进行有效加速，暂时没有深究，也是一个重点。

### Condconv

主要为增加网络性能，增加少量计算量，大量增加模型容量(需要优化)，也比较有前途。

是动态网络的一种，和专家系统的原理非常相似，有点专家系统和动态网络结合的感觉。传统神经网络每一层只有一套固定的卷积核，condconv首先赋予每个卷积层多套权重，然后通过一个门控网络(gate network)来对输入的每一个样本选择不同的专家(expert，这里指代不同的卷积核权重)，通过加权求和得到一个卷积核权重，再将卷积核与样本进行卷积。（这样做的好处就是，只增加了gate网络的计算量，就把整个网络的容量和可表达性进行增加。）

一般来说对于移动端的层数比较低的网络效果比较好，有实验结果，确实有一定效果，参数搜索比较麻烦，计算量要控制好。

论文有三个同时出现的论文，为谷歌 华为研究院和MSRA同时出现，各有优劣，基本原理相同，了解主要看condconv即可，深入都要看

+ CondConv: Conditionally Parameterized Convolutions for Efficient Inference
+ DYNET: DYNAMIC CONVOLUTION FOR ACCELERATING CONVOLUTION NEURAL NETWORKS
+ Dynamic Convolution: Attention over Convolution Kernels

还有一篇改进

+ WeightNet: Revisiting the Design Space of Weight Networks

将condconv和SE-Net结合。

SE-Net是一种使用注意力机制来赋予通道高注意力的网络结构，极其有效，请务必先了解，condconv实际上是用注意力机制在多套卷积核上而不是通道上。

## 已有探索部分

+ Dynamic network和专家系统部分，专家系统需要大量计算能力，几乎无法实验，但是需要完全了解moe的机制才能进行类似研究。我个人做了google 的deep moe(等同于使用专家系统(目的提高性能)来做动态剪枝)，有效果，有图层分析，但是没有看到加速效果，需要深入了解。

+ Condconv这边做了三篇论文的比较实验，设计了一种比较特殊的condconv，有效果但是不稳定，需要重复论证，需要补充在moblienet上面的实验， 如果发论文需要在cifar10 cifar100和Imagenet,初期只需要在cifar10上面做实验。

+ slim还没有进行实验，比较看好。



